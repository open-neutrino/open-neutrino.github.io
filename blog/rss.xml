<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="rss.xsl"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Neutrino Blog</title>
        <link>https://neutrino.github.io/blog</link>
        <description>Neutrino Blog</description>
        <lastBuildDate>Tue, 24 Dec 2024 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[PyTorch One-Line Optimization]]></title>
            <link>https://neutrino.github.io/blog/first-blog-post</link>
            <guid>https://neutrino.github.io/blog/first-blog-post</guid>
            <pubDate>Tue, 24 Dec 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Hello, welcome to Neutrino blogs!]]></description>
            <content:encoded><![CDATA[<p>Hello, welcome to Neutrino blogs!
This blog series will continuously post our new findings on GPU programming and computing via <code>neutrino</code>,
our fine-grained and programmable GPU kernel observability platform like <a href="https://ebpf.io/" target="_blank" rel="noopener noreferrer">eBPF</a> for Linux kernel!</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="observing-pytorch-performance-issue-with-neutrino">Observing PyTorch Performance Issue with Neutrino!<a href="https://neutrino.github.io/blog/first-blog-post#observing-pytorch-performance-issue-with-neutrino" class="hash-link" aria-label="Direct link to Observing PyTorch Performance Issue with Neutrino!" title="Direct link to Observing PyTorch Performance Issue with Neutrino!">‚Äã</a></h3>
<p>As a GPU observability platform, <code>neutrino</code> provides an similar user interface as its CPU siblings <a href="https://man7.org/linux/man-pages/man1/strace.1.html" target="_blank" rel="noopener noreferrer">strace</a>/<a href="https://valgrind.org/" target="_blank" rel="noopener noreferrer">valgrind</a> for profiling nearly any computing workload on GPU, like Deep Learning with PyTorch.
To demonstrate its power, how about let's started with an crazily easy one like the following line:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockTitle_Ktv7">oneline.py</div><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zeros</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">4096</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4096</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float16</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> device</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">"cuda"</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Functionality of the line is straightforward, create a 2D tensor of size <code>4096x4096</code> on GPU filled with zero in type of FP16.
And I believe most of us don't think this line has any potential performance issue, right üßê?
Because it's too straightforward and easy ü´†.</p>
<p>Well computer science is a practical subject so
let's answer this question in the experiemental way with <code>neutrino</code> and more specifically, <code>neutrino</code>'s block scheduling (<code>block_sched</code>) tool:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">neutrino </span><span class="token parameter variable" style="color:#36acaa">-t</span><span class="token plain"> block_sched python oneline.py </span><span class="token comment" style="color:#999988;font-style:italic"># -t to specify tool</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># output (truncated)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># </span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># </span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Here <code>neutrino</code> suggests a kernel named <code>vectorized_elementwise...</code> (weird name comes from C++ template) is executed in <code>torch.zeros</code>.
This behavior is expected as the kernel is used to fill allocated buffer with 0 for correct initialization.
But the poor performance statistics is not expected, the kernel spend more than 25% time on block scheduling! What a waste of time and FLOPs!</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="whats-block-scheduling-cost">What's block scheduling cost?<a href="https://neutrino.github.io/blog/first-blog-post#whats-block-scheduling-cost" class="hash-link" aria-label="Direct link to What's block scheduling cost?" title="Direct link to What's block scheduling cost?">‚Äã</a></h3>
<p>Now it's time for some GPU programming internals.
Most compute platform like NVIDIA CUDA and AMD ROCm formulate GPU threads into blocks like 128 threads per block in this example.
And blocks will be scheduled onto GPUs to run parallely.
But one common blind spot is that parallelism of block-level is not unlimited!</p>
<p>In fact, the parallelism of your kernel (<code>vectorized_elementwise</code> here) is bounded by GPU hardware,
like on my A100 of 108 SM and 6 block/SM (see <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/#occupancy-calculator" target="_blank" rel="noopener noreferrer">CUDA Occupancy Calculator</a>),
only 648 block can be executed parallely, far lower than required 65536.
In such a case, blocks will be executed sequentially other than parallely, i.e., the 649-th block need to wait for an executing block finished to run.</p>
<p>Similar to context switch in CPU, GPU also spends additional time (hundreds of cycles) on finalizing an executing a block and scheduling next block.
And such scheduling time is called <em>block scheduling cost</em> here as the time is wasted from the perspective of computing:</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-neutrino-measure-this">How Neutrino measure this?<a href="https://neutrino.github.io/blog/first-blog-post#how-neutrino-measure-this" class="hash-link" aria-label="Direct link to How Neutrino measure this?" title="Direct link to How Neutrino measure this?">‚Äã</a></h3>
<p>As many of us never heard of this term, block scheduling cost is mostly implicit.
And one possible reason is the difficulty to measure,
because GPU block scheduling is a hardware-behavior, i.e.,
not programmable,
and on the other hand, block execution is programmable but we are trying to measure the hole between blocks!</p>
<p>Neutrino's <code>block_sched</code> tools solve this with a combination approach.
First, in trace collection we collect three runtime statistics for every block being scheduled, starting timestamp(clock), ending timestamp and the scheduled GPU SM id.
With <code>neutrino</code>, this is as simple as following toml:</p>
<pre tabindex="0" class="codeBlockStandalone_MEMb thin-scrollbar language-toml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"></code></pre>
<p>Then, in post-trace analysis we can sort and analyze the scheduling per SM. Block scheduling cost can be estimated as the <u>start clock of next block - end clock of previous block</u>. You can check the code here.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-optimize">How to optimize?<a href="https://neutrino.github.io/blog/first-blog-post#how-to-optimize" class="hash-link" aria-label="Direct link to How to optimize?" title="Direct link to How to optimize?">‚Äã</a></h3>
<p>The best way to verify the correctness of Neutrino observation is to optimize in the way suggested and see if the performance is improved.</p>
<p>Here suggestion given by <code>neutrino</code> is to reduce the block scheduling cost and one straightforward solution is to issue less blocks.
To verify this, we can use Persistent Kernel (like <a href="https://triton-lang.org/main/getting-started/tutorials/09-persistent-matmul.html#sphx-glr-getting-started-tutorials-09-persistent-matmul-py" target="_blank" rel="noopener noreferrer">this example</a>) that removes block scheduling cost via launching a persistent group of threads (at hardware capacity) and manage sequential part manually.</p>
<p>A persistent <code>vectorized_elementwise</code> can be easily written in <a href="https://triton-lang.org/main/index.html" target="_blank" rel="noopener noreferrer">Triton</a> via GPT:</p>
<div class="tabs-container tabList__CuJ"><ul role="tablist" aria-orientation="horizontal" class="tabs"><li role="tab" tabindex="0" aria-selected="true" class="tabs__item tabItem_LNqP tabs__item--active">Short</li><li role="tab" tabindex="-1" aria-selected="false" class="tabs__item tabItem_LNqP">Full</li></ul><div class="margin-top--md"><div role="tabpanel" class="tabItem_Ymn6"><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token decorator annotation punctuation" style="color:#393A34">@triton</span><span class="token decorator annotation punctuation" style="color:#393A34">.</span><span class="token decorator annotation punctuation" style="color:#393A34">jit</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">zero_init_kernel_persistent</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">output_ptr</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> numel</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> BLOCK_SIZE</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">constexpr</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> NUM_SMS</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">constexpr</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    start_pid </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">program_id</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">axis</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_blocks </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cdiv</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">numel</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> BLOCK_SIZE</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    blocks_per_sm </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> num_blocks </span><span class="token operator" style="color:#393A34">//</span><span class="token plain"> NUM_SMS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> start_pid </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> num_blocks </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> NUM_SMS</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        blocks_per_sm </span><span class="token operator" style="color:#393A34">+=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    block_id </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> start_pid </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> NUM_SMS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> _ </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">blocks_per_sm</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        block_id </span><span class="token operator" style="color:#393A34">+=</span><span class="token plain"> NUM_SMS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        offsets </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> block_id </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> BLOCK_SIZE </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">arange</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> BLOCK_SIZE</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        mask </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> offsets </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> numel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">store</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">output_ptr </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> offsets</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zeros</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">BLOCK_SIZE</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float16</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> mask</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">mask</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><div role="tabpanel" class="tabItem_Ymn6" hidden=""><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> torch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> triton</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> triton</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">language </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"> tl</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token decorator annotation punctuation" style="color:#393A34">@triton</span><span class="token decorator annotation punctuation" style="color:#393A34">.</span><span class="token decorator annotation punctuation" style="color:#393A34">jit</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">zero_init_kernel_persistent</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">output_ptr</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> numel</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> BLOCK_SIZE</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">constexpr</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> NUM_SMS</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">constexpr</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Get program ID</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    start_pid </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">program_id</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">axis</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Calculate number of blocks needed</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    num_blocks </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cdiv</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">numel</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> BLOCK_SIZE</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Calculate blocks per SM</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    blocks_per_sm </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> num_blocks </span><span class="token operator" style="color:#393A34">//</span><span class="token plain"> NUM_SMS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> start_pid </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> num_blocks </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> NUM_SMS</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        blocks_per_sm </span><span class="token operator" style="color:#393A34">+=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Initialize block ID</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    block_id </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> start_pid </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> NUM_SMS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Process multiple blocks per SM</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> _ </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">blocks_per_sm</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        block_id </span><span class="token operator" style="color:#393A34">+=</span><span class="token plain"> NUM_SMS</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Calculate offsets for this block</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        offsets </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> block_id </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> BLOCK_SIZE </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">arange</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> BLOCK_SIZE</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Create mask for valid elements</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        mask </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> offsets </span><span class="token operator" style="color:#393A34">&lt;</span><span class="token plain"> numel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token comment" style="color:#999988;font-style:italic"># Store zeros</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">store</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">output_ptr </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> offsets</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zeros</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">BLOCK_SIZE</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">tl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float16</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> mask</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">mask</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">zero_init_persistent</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Tensor</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Get total number of elements</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    numel </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">numel</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Get number of SMs</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    NUM_SMS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuda</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">get_device_properties</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">"cuda"</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">multi_processor_count</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Configure BLOCK_SIZE still at 128</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    BLOCK_SIZE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">128</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Launch kernel</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    grid </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">lambda</span><span class="token plain"> META</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token builtin">min</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">NUM_SMS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> triton</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cdiv</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">numel</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> META</span><span class="token punctuation" style="color:#393A34">[</span><span class="token string" style="color:#e3116c">'BLOCK_SIZE'</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    zero_init_kernel_persistent</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">grid</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> numel</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> BLOCK_SIZE</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">BLOCK_SIZE</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> NUM_SMS</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">NUM_SMS</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> num_warps</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">8</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># Test script</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">if</span><span class="token plain"> __name__ </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">"__main__"</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># Test correctness</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    shape </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">4096</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4096</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    old </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zeros</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">shape</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float16</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> device</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">'cuda'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    new </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">empty</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">shape</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float16</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> device</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">'cuda'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    zero_init_persistent</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">new</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">assert</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">allclose</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">old</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> new</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></div></div>
<p>To apply the change, we can call <code>torch.empty</code> to only allocate memory without initialization and call <code>zero_init_kernel_persistent</code> to initialize manually:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line code-block-remove-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 34,493ns (nsys, 1000 run</span><span class="token plain"></span><br></span><span class="token-line code-block-remove-line" style="color:#393A34"><span class="token plain">old </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zeros</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">shape</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float16</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> device</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">'cuda'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line code-block-add-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># nsys 1000 run: 25ms</span><span class="token plain"></span><br></span><span class="token-line code-block-add-line" style="color:#393A34"><span class="token plain">new </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">empty</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">shape</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float16</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> device</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">'cuda'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line code-block-add-line" style="color:#393A34"><span class="token plain">zero_init_persistent</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">new</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Replacing the line leads to around 33% acceleration, which supports Neutrino's observation of 25% scheduling cost.
Another lazier solution is to use <code>cuMemset</code> driver call which totally eliminates the kernel launch (at least to developers outside Nvidia), which offers similar improvement:</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line code-block-remove-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># 34,493ns (nsys, 1000 run</span><span class="token plain"></span><br></span><span class="token-line code-block-remove-line" style="color:#393A34"><span class="token plain">old </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">zeros</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">shape</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float16</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> device</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">'cuda'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line code-block-add-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic"># 24,630ns (nsys, 1000 run</span><span class="token plain"></span><br></span><span class="token-line code-block-add-line" style="color:#393A34"><span class="token plain">new </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">empty</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">shape</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> dtype</span><span class="token operator" style="color:#393A34">=</span><span class="token plain">torch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">float16</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> device</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">'cuda'</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line code-block-add-line" style="color:#393A34"><span class="token plain">driver</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cuMemsetD16</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">empty</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">data_ptr</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4096</span><span class="token operator" style="color:#393A34">*</span><span class="token number" style="color:#36acaa">4096</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>ü•≥Well Doneü•≥
With <code>neutrino</code>, we observe and accelerate a "mission impossible", a surprisingly easy <code>torch.zeros</code> performance issue that lies in our daily workflow!
Moreover, the underlying <code>vectorized_elementwise</code> not just lies in <code>torch.zeros</code> but also <code>torch.ones/arange/exp</code> (actually any elementwise unary operatorü´£) and even plenty of implicit call to flush implicit buffersü´•!</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="https://neutrino.github.io/blog/first-blog-post#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h3>
<p>In this 5-mins journey, we walk through the basic debug experience of Neutrino with a simple example as one-line of <code>torch.zeros</code>, demonstrating <code>neutrino</code>'s amazing capability to discover performance issue from unexpected perspective.</p>
<p>Finally I want to conclude with an inspiring question:</p>
<blockquote>
<p>Even such a simple line can have such a big issue, how many potential performance issue in current GPU can we explore and optimize with Neutrino?</p>
</blockquote>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="further-reading">Further Reading<a href="https://neutrino.github.io/blog/first-blog-post#further-reading" class="hash-link" aria-label="Direct link to Further Reading" title="Direct link to Further Reading">‚Äã</a></h3>
<p>If you are interested in <code>neutrino</code>, take a look at:</p>
<ul>
<li><a href="https://neutrino.github.io/docs/intro">Introduction</a>: More detailed and insightful information of Neutrino</li>
<li>Probe Design: How tools like <code>block_sched</code> is designed and implemented!</li>
<li>Our later blog!</li>
</ul>]]></content:encoded>
            <category>Introduction</category>
        </item>
    </channel>
</rss>