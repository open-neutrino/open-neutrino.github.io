"use strict";(self.webpackChunkneutrino=self.webpackChunkneutrino||[]).push([[1831],{3679:e=>{e.exports=JSON.parse('{"permalink":"/blog/first-blog-post","editUrl":"https://github.com/neutrino-gpu/neutrino/docs/blog/2024-12-24-5min/index.mdx","source":"@site/blog/2024-12-24-5min/index.mdx","title":"PyTorch One-Line Optimization","description":"Hello, welcome to Neutrino blogs!","date":"2024-12-24T00:00:00.000Z","tags":[{"inline":false,"label":"Introduction","permalink":"/blog/tags/hello","description":"Blogs introducing Neutrino basics"}],"readingTime":6.055,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"first-blog-post","title":"PyTorch One-Line Optimization","tags":["hello"]},"unlisted":false}')},5106:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>c,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>d});var r=t(3679),o=t(4848),s=t(8453),i=t(5537),l=t(9329);const a={slug:"first-blog-post",title:"PyTorch One-Line Optimization",tags:["hello"]},c=void 0,u={authorsImageUrls:[]},d=[{value:"Observing PyTorch Performance Issue with Neutrino!",id:"observing-pytorch-performance-issue-with-neutrino",level:3},{value:"What&#39;s block scheduling cost?",id:"whats-block-scheduling-cost",level:3},{value:"How Neutrino measure this?",id:"how-neutrino-measure-this",level:3},{value:"How to optimize?",id:"how-to-optimize",level:3},{value:"Conclusion",id:"conclusion",level:3},{value:"Further Reading",id:"further-reading",level:3}];function h(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["Hello, welcome to Neutrino blogs!\nThis blog series will continuously post our new findings on GPU programming and computing via ",(0,o.jsx)(n.code,{children:"neutrino"}),",\nour fine-grained and programmable GPU kernel observability platform like ",(0,o.jsx)(n.a,{href:"https://ebpf.io/",children:"eBPF"})," for Linux kernel!"]}),"\n",(0,o.jsx)(n.h3,{id:"observing-pytorch-performance-issue-with-neutrino",children:"Observing PyTorch Performance Issue with Neutrino!"}),"\n",(0,o.jsxs)(n.p,{children:["As a GPU observability platform, ",(0,o.jsx)(n.code,{children:"neutrino"})," provides an similar user interface as its CPU siblings ",(0,o.jsx)(n.a,{href:"https://man7.org/linux/man-pages/man1/strace.1.html",children:"strace"}),"/",(0,o.jsx)(n.a,{href:"https://valgrind.org/",children:"valgrind"})," for profiling nearly any computing workload on GPU, like Deep Learning with PyTorch.\nTo demonstrate its power, how about let's started with an crazily easy one like the following line:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",metastring:'title="oneline.py"',children:'torch.zeros((4096, 4096), dtype=torch.float16, device="cuda")\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Functionality of the line is straightforward, create a 2D tensor of size ",(0,o.jsx)(n.code,{children:"4096x4096"})," on GPU filled with zero in type of FP16.\nAnd I believe most of us don't think this line has any potential performance issue, right \ud83e\uddd0?\nBecause it's too straightforward and easy \ud83e\udee0."]}),"\n",(0,o.jsxs)(n.p,{children:["Well computer science is a practical subject so\nlet's answer this question in the experiemental way with ",(0,o.jsx)(n.code,{children:"neutrino"})," and more specifically, ",(0,o.jsx)(n.code,{children:"neutrino"}),"'s block scheduling (",(0,o.jsx)(n.code,{children:"block_sched"}),") tool:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"neutrino -t block_sched python oneline.py # -t to specify tool\n# output (truncated)\n# \n# \n# \n"})}),"\n",(0,o.jsxs)(n.p,{children:["Here ",(0,o.jsx)(n.code,{children:"neutrino"})," suggests a kernel named ",(0,o.jsx)(n.code,{children:"vectorized_elementwise..."})," (weird name comes from C++ template) is executed in ",(0,o.jsx)(n.code,{children:"torch.zeros"}),".\nThis behavior is expected as the kernel is used to fill allocated buffer with 0 for correct initialization.\nBut the poor performance statistics is not expected, the kernel spend more than 25% time on block scheduling! What a waste of time and FLOPs!"]}),"\n",(0,o.jsx)(n.h3,{id:"whats-block-scheduling-cost",children:"What's block scheduling cost?"}),"\n",(0,o.jsx)(n.p,{children:"Now it's time for some GPU programming internals.\nMost compute platform like NVIDIA CUDA and AMD ROCm formulate GPU threads into blocks like 128 threads per block in this example.\nAnd blocks will be scheduled onto GPUs to run parallely.\nBut one common blind spot is that parallelism of block-level is not unlimited!"}),"\n",(0,o.jsxs)(n.p,{children:["In fact, the parallelism of your kernel (",(0,o.jsx)(n.code,{children:"vectorized_elementwise"})," here) is bounded by GPU hardware,\nlike on my A100 of 108 SM and 6 block/SM (see ",(0,o.jsx)(n.a,{href:"https://docs.nvidia.com/cuda/cuda-c-programming-guide/#occupancy-calculator",children:"CUDA Occupancy Calculator"}),"),\nonly 648 block can be executed parallely, far lower than required 65536.\nIn such a case, blocks will be executed sequentially other than parallely, i.e., the 649-th block need to wait for an executing block finished to run."]}),"\n",(0,o.jsxs)(n.p,{children:["Similar to context switch in CPU, GPU also spends additional time (hundreds of cycles) on finalizing an executing a block and scheduling next block.\nAnd such scheduling time is called ",(0,o.jsx)(n.em,{children:"block scheduling cost"})," here as the time is wasted from the perspective of computing:"]}),"\n",(0,o.jsx)(n.h3,{id:"how-neutrino-measure-this",children:"How Neutrino measure this?"}),"\n",(0,o.jsx)(n.p,{children:"As many of us never heard of this term, block scheduling cost is mostly implicit.\nAnd one possible reason is the difficulty to measure,\nbecause GPU block scheduling is a hardware-behavior, i.e.,\nnot programmable,\nand on the other hand, block execution is programmable but we are trying to measure the hole between blocks!"}),"\n",(0,o.jsxs)(n.p,{children:["Neutrino's ",(0,o.jsx)(n.code,{children:"block_sched"})," tools solve this with a combination approach.\nFirst, in trace collection we collect three runtime statistics for every block being scheduled, starting timestamp(clock), ending timestamp and the scheduled GPU SM id.\nWith ",(0,o.jsx)(n.code,{children:"neutrino"}),", this is as simple as following toml:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-toml"})}),"\n",(0,o.jsxs)(n.p,{children:["Then, in post-trace analysis we can sort and analyze the scheduling per SM. Block scheduling cost can be estimated as the ",(0,o.jsx)("u",{children:"start clock of next block - end clock of previous block"}),". You can check the code here."]}),"\n",(0,o.jsx)(n.h3,{id:"how-to-optimize",children:"How to optimize?"}),"\n",(0,o.jsx)(n.p,{children:"The best way to verify the correctness of Neutrino observation is to optimize in the way suggested and see if the performance is improved."}),"\n",(0,o.jsxs)(n.p,{children:["Here suggestion given by ",(0,o.jsx)(n.code,{children:"neutrino"})," is to reduce the block scheduling cost and one straightforward solution is to issue less blocks.\nTo verify this, we can use Persistent Kernel (like ",(0,o.jsx)(n.a,{href:"https://triton-lang.org/main/getting-started/tutorials/09-persistent-matmul.html#sphx-glr-getting-started-tutorials-09-persistent-matmul-py",children:"this example"}),") that removes block scheduling cost via launching a persistent group of threads (at hardware capacity) and manage sequential part manually."]}),"\n",(0,o.jsxs)(n.p,{children:["A persistent ",(0,o.jsx)(n.code,{children:"vectorized_elementwise"})," can be easily written in ",(0,o.jsx)(n.a,{href:"https://triton-lang.org/main/index.html",children:"Triton"})," via GPT:"]}),"\n",(0,o.jsxs)(i.A,{children:[(0,o.jsx)(l.A,{value:"short",label:"Short",children:(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"@triton.jit\ndef zero_init_kernel_persistent(output_ptr, numel, BLOCK_SIZE: tl.constexpr, NUM_SMS: tl.constexpr):\n    start_pid = tl.program_id(axis=0)\n    num_blocks = tl.cdiv(numel, BLOCK_SIZE)\n    blocks_per_sm = num_blocks // NUM_SMS\n    if start_pid < num_blocks % NUM_SMS:\n        blocks_per_sm += 1\n    block_id = start_pid - NUM_SMS\n    for _ in range(blocks_per_sm):\n        block_id += NUM_SMS\n        offsets = block_id * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < numel\n        tl.store(output_ptr + offsets, tl.zeros([BLOCK_SIZE], dtype=tl.float16), mask=mask)\n"})})}),(0,o.jsx)(l.A,{value:"long",label:"Full",children:(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import torch\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef zero_init_kernel_persistent(output_ptr, numel, BLOCK_SIZE: tl.constexpr, NUM_SMS: tl.constexpr):\n    # Get program ID\n    start_pid = tl.program_id(axis=0)\n    # Calculate number of blocks needed\n    num_blocks = tl.cdiv(numel, BLOCK_SIZE)\n    # Calculate blocks per SM\n    blocks_per_sm = num_blocks // NUM_SMS\n    if start_pid < num_blocks % NUM_SMS:\n        blocks_per_sm += 1\n    # Initialize block ID\n    block_id = start_pid - NUM_SMS\n    \n    # Process multiple blocks per SM\n    for _ in range(blocks_per_sm):\n        block_id += NUM_SMS\n        # Calculate offsets for this block\n        offsets = block_id * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        # Create mask for valid elements\n        mask = offsets < numel\n        # Store zeros\n        tl.store(output_ptr + offsets, tl.zeros([BLOCK_SIZE], dtype=tl.float16), mask=mask)\n\ndef zero_init_persistent(x: torch.Tensor):\n    # Get total number of elements\n    numel = x.numel()\n    # Get number of SMs\n    NUM_SMS = torch.cuda.get_device_properties(\"cuda\").multi_processor_count\n    # Configure BLOCK_SIZE still at 128\n    BLOCK_SIZE = 128\n    # Launch kernel\n    grid = lambda META: (min(NUM_SMS, triton.cdiv(numel, META['BLOCK_SIZE'])),)\n    zero_init_kernel_persistent[grid](x, numel, BLOCK_SIZE=BLOCK_SIZE, NUM_SMS=NUM_SMS, num_warps=8)\n\n# Test script\nif __name__ == \"__main__\":\n    # Test correctness\n    shape = (4096, 4096)\n    old = torch.zeros(shape, dtype=torch.float16, device='cuda')\n    new = torch.empty(shape, dtype=torch.float16, device='cuda')\n    zero_init_persistent(new)\n    assert torch.allclose(old, new)\n"})})})]}),"\n",(0,o.jsxs)(n.p,{children:["To apply the change, we can call ",(0,o.jsx)(n.code,{children:"torch.empty"})," to only allocate memory without initialization and call ",(0,o.jsx)(n.code,{children:"zero_init_kernel_persistent"})," to initialize manually:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# REMOVE NEXT LINE\n# 34,493ns (nsys, 1000 run\n# REMOVE NEXT LINE\nold = torch.zeros(shape, dtype=torch.float16, device='cuda')\n# ADD NEXT LINE\n# nsys 1000 run: 25ms\n# ADD NEXT LINE\nnew = torch.empty(shape, dtype=torch.float16, device='cuda')\n# ADD NEXT LINE\nzero_init_persistent(new)\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Replacing the line leads to around 33% acceleration, which supports Neutrino's observation of 25% scheduling cost.\nAnother lazier solution is to use ",(0,o.jsx)(n.code,{children:"cuMemset"})," driver call which totally eliminates the kernel launch (at least to developers outside Nvidia), which offers similar improvement:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# REMOVE NEXT LINE\n# 34,493ns (nsys, 1000 run\n# REMOVE NEXT LINE\nold = torch.zeros(shape, dtype=torch.float16, device='cuda')\n# ADD NEXT LINE\n# 24,630ns (nsys, 1000 run\n# ADD NEXT LINE\nnew = torch.empty(shape, dtype=torch.float16, device='cuda')\n# ADD NEXT LINE\ndriver.cuMemsetD16(empty.data_ptr(), 0, 4096*4096)\n"})}),"\n",(0,o.jsxs)(n.p,{children:["\ud83e\udd73Well Done\ud83e\udd73\nWith ",(0,o.jsx)(n.code,{children:"neutrino"}),', we observe and accelerate a "mission impossible", a surprisingly easy ',(0,o.jsx)(n.code,{children:"torch.zeros"})," performance issue that lies in our daily workflow!\nMoreover, the underlying ",(0,o.jsx)(n.code,{children:"vectorized_elementwise"})," not just lies in ",(0,o.jsx)(n.code,{children:"torch.zeros"})," but also ",(0,o.jsx)(n.code,{children:"torch.ones/arange/exp"})," (actually any elementwise unary operator\ud83e\udee3) and even plenty of implicit call to flush implicit buffers\ud83e\udee5!"]}),"\n",(0,o.jsx)(n.h3,{id:"conclusion",children:"Conclusion"}),"\n",(0,o.jsxs)(n.p,{children:["In this 5-mins journey, we walk through the basic debug experience of Neutrino with a simple example as one-line of ",(0,o.jsx)(n.code,{children:"torch.zeros"}),", demonstrating ",(0,o.jsx)(n.code,{children:"neutrino"}),"'s amazing capability to discover performance issue from unexpected perspective."]}),"\n",(0,o.jsx)(n.p,{children:"Finally I want to conclude with an inspiring question:"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsx)(n.p,{children:"Even such a simple line can have such a big issue, how many potential performance issue in current GPU can we explore and optimize with Neutrino?"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(n.p,{children:["If you are interested in ",(0,o.jsx)(n.code,{children:"neutrino"}),", take a look at:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/docs/intro",children:"Introduction"}),": More detailed and insightful information of Neutrino"]}),"\n",(0,o.jsxs)(n.li,{children:["Probe Design: How tools like ",(0,o.jsx)(n.code,{children:"block_sched"})," is designed and implemented!"]}),"\n",(0,o.jsx)(n.li,{children:"Our later blog!"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},5537:(e,n,t)=>{t.d(n,{A:()=>k});var r=t(6540),o=t(4164),s=t(5627),i=t(6347),l=t(372),a=t(604),c=t(1861),u=t(8749);function d(e){return r.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function h(e){const{values:n,children:t}=e;return(0,r.useMemo)((()=>{const e=n??function(e){return d(e).map((e=>{let{props:{value:n,label:t,attributes:r,default:o}}=e;return{value:n,label:t,attributes:r,default:o}}))}(t);return function(e){const n=(0,c.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function p(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function m(e){let{queryString:n=!1,groupId:t}=e;const o=(0,i.W6)(),s=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,a.aZ)(s),(0,r.useCallback)((e=>{if(!s)return;const n=new URLSearchParams(o.location.search);n.set(s,e),o.replace({...o.location,search:n.toString()})}),[s,o])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:o}=e,s=h(e),[i,a]=(0,r.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!p({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const r=t.find((e=>e.default))??t[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:s}))),[c,d]=m({queryString:t,groupId:o}),[f,b]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[o,s]=(0,u.Dv)(t);return[o,(0,r.useCallback)((e=>{t&&s.set(e)}),[t,s])]}({groupId:o}),g=(()=>{const e=c??f;return p({value:e,tabValues:s})?e:null})();(0,l.A)((()=>{g&&a(g)}),[g]);return{selectedValue:i,selectValue:(0,r.useCallback)((e=>{if(!p({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);a(e),d(e),b(e)}),[d,b,s]),tabValues:s}}var b=t(9136);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var x=t(4848);function y(e){let{className:n,block:t,selectedValue:r,selectValue:i,tabValues:l}=e;const a=[],{blockElementScrollPositionUntilNextRender:c}=(0,s.a_)(),u=e=>{const n=e.currentTarget,t=a.indexOf(n),o=l[t].value;o!==r&&(c(n),i(o))},d=e=>{let n=null;switch(e.key){case"Enter":u(e);break;case"ArrowRight":{const t=a.indexOf(e.currentTarget)+1;n=a[t]??a[0];break}case"ArrowLeft":{const t=a.indexOf(e.currentTarget)-1;n=a[t]??a[a.length-1];break}}n?.focus()};return(0,x.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":t},n),children:l.map((e=>{let{value:n,label:t,attributes:s}=e;return(0,x.jsx)("li",{role:"tab",tabIndex:r===n?0:-1,"aria-selected":r===n,ref:e=>{a.push(e)},onKeyDown:d,onClick:u,...s,className:(0,o.A)("tabs__item",g.tabItem,s?.className,{"tabs__item--active":r===n}),children:t??n},n)}))})}function v(e){let{lazy:n,children:t,selectedValue:s}=e;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=i.find((e=>e.props.value===s));return e?(0,r.cloneElement)(e,{className:(0,o.A)("margin-top--md",e.props.className)}):null}return(0,x.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==s})))})}function _(e){const n=f(e);return(0,x.jsxs)("div",{className:(0,o.A)("tabs-container",g.tabList),children:[(0,x.jsx)(y,{...n,...e}),(0,x.jsx)(v,{...n,...e})]})}function k(e){const n=(0,b.A)();return(0,x.jsx)(_,{...e,children:d(e.children)},String(n))}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>l});var r=t(6540);const o={},s=r.createContext(o);function i(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),r.createElement(s.Provider,{value:n},e.children)}},9329:(e,n,t)=>{t.d(n,{A:()=>i});t(6540);var r=t(4164);const o={tabItem:"tabItem_Ymn6"};var s=t(4848);function i(e){let{children:n,hidden:t,className:i}=e;return(0,s.jsx)("div",{role:"tabpanel",className:(0,r.A)(o.tabItem,i),hidden:t,children:n})}}}]);